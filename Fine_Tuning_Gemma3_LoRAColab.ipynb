{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDEExiAk4fLb"
   },
   "source": [
    "# üöÄ Fine-Tuning Gemma 3 270M (or Higher) on CPU/GPU with LoRA\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DUnWNxLMxL3WprUqhyLayPxVr5KffTsC?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UWUWyYaQc1j"
   },
   "source": [
    "## üìñ About This Tutorial\n",
    "\n",
    "### üë• Who Is This For?\n",
    "\n",
    "This comprehensive guide is designed for **everyone** interested in fine-tuning language models:\n",
    "\n",
    "- üéì **Students & Researchers**: Learn the complete workflow from data creation to model deployment\n",
    "- üíº **ML Engineers**: Understand practical implementation details and best practices\n",
    "- üöÄ **AI Enthusiasts**: Get hands-on experience with state-of-the-art fine-tuning techniques\n",
    "- üè¢ **Domain Experts**: Adapt language models to your specific field (medical, legal, scientific, etc.)\n",
    "\n",
    "**No prior fine-tuning experience required!** We'll walk through everything step-by-step.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "\n",
    "In this tutorial, we'll fine-tune **Gemma 3 270M** on a custom radiobiology dataset. You'll master:\n",
    "\n",
    "1. üìä **Data Creation**: Generate high-quality training data from scratch using LLM-DATA-Generator\n",
    "2. üîß **Model Setup**: Load and configure Gemma 3 models with KerasHub\n",
    "3. üéõÔ∏è **LoRA Fine-Tuning**: Apply parameter-efficient fine-tuning techniques\n",
    "4. üìà **Training**: Optimize hyperparameters and monitor training progress\n",
    "5. üß™ **Evaluation**: Test model performance before and after fine-tuning\n",
    "6. üíæ **Deployment**: Save and use your fine-tuned model for inference\n",
    "\n",
    "---\n",
    "\n",
    "### üíª Hardware Requirements: CPU-First Approach\n",
    "\n",
    "#### üñ•Ô∏è This Tutorial Uses CPU\n",
    "\n",
    "We'll be fine-tuning **Gemma 3 270M on CPU** to make this tutorial **accessible to everyone**!\n",
    "\n",
    "‚úÖ **No expensive GPU required**  \n",
    "‚úÖ **Runs on any laptop or desktop**  \n",
    "‚úÖ **Perfect for learning and experimentation**  \n",
    "‚úÖ **Same code works everywhere**\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ Want Faster Training? Use the Same Code!\n",
    "\n",
    "The beauty of this tutorial is that **the exact same code** can run on different hardware:\n",
    "\n",
    "**Option 1: CPU (What We're Using)** üñ•Ô∏è\n",
    "- ‚úÖ Works on any device\n",
    "- ‚úÖ No special setup required\n",
    "- ‚úÖ Great for learning\n",
    "- ‚è±Ô∏è Training time: Slower but totally doable for Gemma 270M\n",
    "\n",
    "**Option 2: GPU (Same Code, Faster!)** ‚ö°\n",
    "- ‚úÖ Just install GPU-enabled packages:\n",
    "  ```bash\n",
    "  pip install keras-hub[jax] jax[cuda12]  # For NVIDIA GPUs\n",
    "  ```\n",
    "- ‚úÖ **No code changes needed** - it automatically detects your GPU!\n",
    "- ‚ö° Training time: Much faster (minutes instead of hours)\n",
    "- üí° Recommended for larger models or frequent fine-tuning\n",
    "\n",
    "**Option 3: Google Colab (Free GPU!)** üåü\n",
    "- ‚úÖ Upload this notebook to [Google Colab](https://colab.research.google.com/)\n",
    "- ‚úÖ Runtime ‚Üí Change runtime type ‚Üí GPU (T4 with 15GB VRAM)\n",
    "- ‚úÖ **Same code, zero installation, free GPU access!**\n",
    "- üí° Perfect if you want speed without buying hardware\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üó∫Ô∏è The Complete Workflow\n",
    "\n",
    "Here's what we'll accomplish in this tutorial:\n",
    "\n",
    "```\n",
    "üìö Step 1: Data Creation\n",
    "   ‚îú‚îÄ Source: Radiobiology textbook (PDF)\n",
    "   ‚îú‚îÄ Tool: LLM-DATA-Generator\n",
    "   ‚îî‚îÄ Output: 832 Q&A pairs (CSV)\n",
    "   \n",
    "‚¨áÔ∏è\n",
    "\n",
    "üîß Step 2: Environment Setup\n",
    "   ‚îú‚îÄ Install Keras & KerasHub\n",
    "   ‚îú‚îÄ Configure Kaggle credentials\n",
    "   ‚îî‚îÄ Set JAX backend for optimal performance\n",
    "   \n",
    "‚¨áÔ∏è\n",
    "\n",
    "ü§ñ Step 3: Model Loading\n",
    "   ‚îú‚îÄ Download Gemma 3 270M from Kaggle\n",
    "   ‚îú‚îÄ Load pre-trained weights\n",
    "   ‚îî‚îÄ Test baseline performance\n",
    "   \n",
    "‚¨áÔ∏è\n",
    "\n",
    "üéØ Step 4: LoRA Configuration\n",
    "   ‚îú‚îÄ Enable LoRA (rank=8)\n",
    "   ‚îú‚îÄ Freeze base model weights\n",
    "   ‚îî‚îÄ Configure trainable parameters\n",
    "   \n",
    "‚¨áÔ∏è\n",
    "\n",
    "üìà Step 5: Training\n",
    "   ‚îú‚îÄ Prepare dataset\n",
    "   ‚îú‚îÄ Set hyperparameters\n",
    "   ‚îî‚îÄ Train the model\n",
    "   \n",
    "‚¨áÔ∏è\n",
    "\n",
    "‚úÖ Step 6: Evaluation & Inference\n",
    "   ‚îú‚îÄ Compare before/after performance\n",
    "   ‚îú‚îÄ Test on domain-specific questions\n",
    "   ‚îî‚îÄ Save fine-tuned model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è±Ô∏è Time Commitment\n",
    "\n",
    "- **Reading & Understanding**: ~30 minutes\n",
    "- **Running the Code** (with GPU): ~45-60 minutes\n",
    "- **Total**: ~1.5-2 hours for complete mastery\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ What's Included\n",
    "\n",
    "- ‚úÖ Complete, runnable code with detailed explanations\n",
    "- ‚úÖ Pre-generated dataset (832 Q&A pairs)\n",
    "- ‚úÖ Step-by-step instructions with visual aids\n",
    "- ‚úÖ Best practices and optimization tips\n",
    "- ‚úÖ Troubleshooting guidance\n",
    "- ‚úÖ Links to additional resources\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Ready to Get Started?\n",
    "\n",
    "Let's dive into the fine-tuning architecture and understand how all the pieces fit together!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSGRSsRPgkzK"
   },
   "source": [
    "\n",
    "\n",
    "## üèóÔ∏è The Fine-Tuning Architecture\n",
    "\n",
    "### üß† Pop Quiz #3: Understanding the Flow\n",
    "\n",
    "**Question**: In what order should we fine-tune a model?\n",
    "\n",
    "- A) Train first, then worry about data later ü§™\n",
    "- B) Get data ‚Üí Load model ‚Üí Train ‚Üí Evaluate üéØ\n",
    "- C) Just hope for the best üôè\n",
    "- D) Download model ‚Üí Pray to the AI gods üïØÔ∏è\n",
    "\n",
    "<details>\n",
    "<summary><b>üëâ Click for the answer!</b></summary>\n",
    "\n",
    "**Answer: B!** Let's see the architecture:\n",
    "\n",
    "</details>\n",
    "\n",
    "![Fine-Tuning Architecture](Architecture.png)\n",
    "\n",
    "### Understanding the Architecture üé®\n",
    "\n",
    "The diagram above shows our complete fine-tuning pipeline.\n",
    "   \n",
    "\n",
    "## üìä Step 1: Data Generation - Creating Your Training Dataset\n",
    "\n",
    "### The Challenge ü§î\n",
    "\n",
    "**Quick question**: How long would it take YOU to manually create 1,000 high-quality Q&A pairs from a textbook?\n",
    "\n",
    "- A) A few hours ‚è∞\n",
    "- B) A few days üìÖ\n",
    "- C) A few weeks üò∞\n",
    "- D) I'd rather not... üíÄ\n",
    "\n",
    "**Answer**: Probably C or D! That's why we automate it. ü§ñ\n",
    "\n",
    "### Solution: LLM-DATA-Generator ‚ö°\n",
    "\n",
    "We use the **LLM-DATA-Generator** tool to automatically generate training data from the Radiobiology textbook.\n",
    "\n",
    "**üîó Tool Repository**: [ElHadheqMind/LLM-DATA-Generator](https://github.com/ElHadheqMind/LLM-DATA-Generator)\n",
    "\n",
    "### How It Works (The Simple Version)\n",
    "\n",
    "```\n",
    "üìñ Input: Radiobiology Textbook (PDF)\n",
    "    ‚Üì\n",
    "ü§ñ LLM-DATA-Generator Magic\n",
    "    ‚Üì\n",
    "üìä Output: 832 Q&A Pairs (CSV)\n",
    "    ‚Üì\n",
    "‚úÖ Ready for Fine-Tuning!\n",
    "```\n",
    "\n",
    "**Time saved**: ~2 weeks of manual work ‚Üí 30 minutes automated! üéâ\n",
    "\n",
    "---\n",
    "\n",
    "### üé¨ Dataset Generation Process in Action\n",
    "\n",
    "Watch this step-by-step walkthrough showing how to generate the dataset using LLM-DATA-Generator:\n",
    "\n",
    "![Dataset Generation Demo](llm-data-generator-demo.gif)\n",
    "\n",
    "**What the video covers**:\n",
    "1. ‚öôÔ∏è **Setup**: Installing and configuring LLM-DATA-Generator\n",
    "2. üîë **API Configuration**: Setting up LLM API credentials (OpenAI/Anthropic/Google)\n",
    "3. üìÑ **Document Upload**: Loading the source textbook (PDF/DOCX)\n",
    "4. üéõÔ∏è **Parameter Tuning**: Configuring generation settings (question count, difficulty, format)\n",
    "5. üöÄ **Generation**: Running the automated Q&A extraction process\n",
    "6. ‚úÖ **Export**: Reviewing and exporting the final CSV dataset\n",
    "\n",
    "**üîó Tool Repository**: [ElHadheqMind/LLM-DATA-Generator](https://github.com/ElHadheqMind/LLM-DATA-Generator)\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "Our generated dataset contains:\n",
    "\n",
    "| Column | Description | Example |\n",
    "|--------|-------------|----------|\n",
    "| **Question** | Domain-specific question | \"What is the primary mechanism by which radiation damages DNA?\" |\n",
    "| **Answer** | Comprehensive answer with context | \"Radiation damages DNA primarily through...\" |\n",
    "| **Content** | Source text from the textbook | Original section from the book |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyhHCMfoRZ_v"
   },
   "source": [
    "## Setup\n",
    "\n",
    "To complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
    "\n",
    "* Get access to Gemma on [kaggle.com](https://kaggle.com).\n",
    "* Optional: GPU resources (RTX 4060 8GB or similar) for faster training, but **CPU works fine** for this tutorial with Gemma 3 270M.\n",
    "  [Learn more](https://ai.google.dev/gemma/docs/core#sizes).\n",
    "* Generate and configure a Kaggle username and API key.\n",
    "\n",
    "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsPC0HRkJl0K"
   },
   "source": [
    "### üîë Kaggle Configuration\n",
    "\n",
    "Gemma models are hosted on Kaggle. You'll need API credentials to download them programmatically.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Create Kaggle Account & Accept Terms**\n",
    "\n",
    "1. **Sign up**: [kaggle.com/account/login](https://www.kaggle.com/account/login?phase=startRegisterTab)\n",
    "2. **Accept Gemma license**: [kaggle.com/models/google/gemma-3](https://www.kaggle.com/models/google/gemma-3)  \n",
    "   Click any variant ‚Üí **\"Request Access\"** ‚Üí Accept terms\n",
    "\n",
    "**üìö Reference**: [Kaggle Models Documentation](https://www.kaggle.com/docs/models)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Generate API Token**\n",
    "\n",
    "1. Profile ‚Üí **Settings** ‚Üí Scroll to **API** section\n",
    "2. Click **\"Create New Token\"** ‚Üí Downloads `kaggle.json`\n",
    "\n",
    "**File structure**:\n",
    "```json\n",
    "{\"username\": \"your_username\", \"key\": \"abc123...\"}\n",
    "```\n",
    "\n",
    "**üîí Security**: This file grants full API access. Never commit to Git or share publicly.\n",
    "\n",
    "**üìö Reference**: [Kaggle API Authentication](https://www.kaggle.com/docs/api#authentication)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iOF6Yo-wUEC"
   },
   "source": [
    "### üîê Load Kaggle Credentials\n",
    "\n",
    "This cell loads your Kaggle credentials and sets the required environment variables (`KAGGLE_USERNAME` and `KAGGLE_KEY`).\n",
    "\n",
    "**How it works**: The Kaggle Python library checks these environment variables when downloading models. See [kaggle-api source](https://github.com/Kaggle/kaggle-api/blob/main/kaggle/api_client.py#L53)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_EdOg9DPK6Q",
    "outputId": "7bbd14b5-3d6b-4175-ecbd-8fd33170a867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kaggle credentials loaded from kaggle.json\n",
      "   Username: mezzihoussem\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# Reads credentials from a local kaggle.json file in the current directory.\n",
    "# This approach keeps credentials out of code and works across environments.\n",
    "# ============================================================================\n",
    "\n",
    "with open('kaggle.json', 'r') as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = kaggle_creds['username']\n",
    "os.environ[\"KAGGLE_KEY\"] = kaggle_creds['key']\n",
    "\n",
    "# Optional: Set additional Kaggle environment variables\n",
    "# os.environ[\"KAGGLE_PROXY\"] = \"http://proxy.example.com:8080\"  # For corporate proxies\n",
    "# os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/custom/path/.kaggle\"      # Override config directory\n",
    "\n",
    "print(\"‚úÖ Kaggle credentials loaded from kaggle.json\")\n",
    "print(f\"   Username: {kaggle_creds['username']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuEUAKJW1QkQ"
   },
   "source": [
    "### üì¶ Install Required Packages\n",
    "\n",
    "We'll install the latest versions of **Keras** and **KerasHub**:\n",
    "\n",
    "- **Keras 3**: Multi-framework deep learning API (supports JAX, TensorFlow, PyTorch)\n",
    "- **KerasHub**: Pre-trained models and utilities for NLP tasks, including Gemma\n",
    "\n",
    "These packages provide:\n",
    "- Easy access to Gemma models via `from_preset()`\n",
    "- Built-in LoRA support with `enable_lora()`\n",
    "- Efficient training utilities and optimizers\n",
    "- Seamless integration with different backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eeBtYqJsZPG",
    "outputId": "b10a6ca4-dcfe-4aa3-b0f0-251ad7fed3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PACKAGE INSTALLATION\n",
    "# ============================================================================\n",
    "# Install KerasHub (includes pre-trained models) and Keras 3\n",
    "# The -q flag suppresses verbose output, -U ensures latest versions\n",
    "# ============================================================================\n",
    "\n",
    "!pip install -q -U keras-hub\n",
    "!pip install -q -U keras\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGLS-l5TxIR4"
   },
   "source": [
    "### ‚öôÔ∏è Select Keras Backend\n",
    "\n",
    "**Keras 3** is a multi-framework API that supports three backends:\n",
    "\n",
    "| Backend | Pros | Cons | Best For |\n",
    "|---------|------|------|----------|\n",
    "| **JAX** | Fastest training, XLA compilation, best GPU utilization | Newer ecosystem | Production training |\n",
    "| **TensorFlow** | Mature ecosystem, TensorFlow Serving | Slower than JAX | Deployment |\n",
    "| **PyTorch** | Popular, extensive community | Slower than JAX | Research |\n",
    "\n",
    "**For this tutorial**, we use **JAX** because:\n",
    "- ‚úÖ Best performance for LoRA fine-tuning\n",
    "- ‚úÖ Optimized for NVIDIA GPUs (including RTX 4060)\n",
    "- ‚úÖ Recommended by Google for Gemma models\n",
    "- ‚úÖ Efficient memory management for 8GB VRAM\n",
    "\n",
    "**Reference**: [Keras 3 Multi-Backend Documentation](https://keras.io/keras_3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn5uy8X8sdD0",
    "outputId": "8065477c-529b-41b8-e572-8a1d416014f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backend configured: JAX\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BACKEND CONFIGURATION\n",
    "# ============================================================================\n",
    "# Set JAX as the Keras backend for optimal performance\n",
    "# XLA_PYTHON_CLIENT_MEM_FRACTION controls GPU memory allocation (1.00 = 100%)\n",
    "# This prevents memory fragmentation on JAX backend\n",
    "# ============================================================================\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Options: \"jax\", \"tensorflow\", \"torch\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"  # Use full GPU memory\n",
    "\n",
    "print(\"‚úÖ Backend configured: JAX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZs8XXqUKRmi"
   },
   "source": [
    "### üìö Import Libraries\n",
    "\n",
    "Now we'll import the necessary libraries:\n",
    "\n",
    "- **keras**: Core deep learning framework\n",
    "- **keras_hub**: Pre-trained models and NLP utilities\n",
    "- **pandas**: For dataset manipulation\n",
    "- **numpy**: For numerical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYHyPUA9hKTf",
    "outputId": "729cc057-0592-47d1-9fa7-8605bb3513b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keras version: 3.12.0\n",
      "‚úÖ KerasHub version: 0.23.0\n",
      "‚úÖ Backend: jax\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LIBRARY IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print(f\"‚úÖ Keras version: {keras.__version__}\")\n",
    "print(f\"‚úÖ KerasHub version: {keras_hub.__version__}\")\n",
    "print(f\"‚úÖ Backend: {keras.backend.backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "version_check"
   },
   "source": [
    "### üìã Verify Package Versions\n",
    "\n",
    "Let's verify that all packages are installed with the correct versions. This tutorial was tested with the following configuration:\n",
    "\n",
    "**Tested Configuration:**\n",
    "- **Keras**: 3.12.0\n",
    "- **KerasHub**: 0.23.0\n",
    "- **TensorFlow**: 2.19.0\n",
    "- **JAX**: 0.7.2\n",
    "- **TensorFlow Text**: 2.19.0\n",
    "- **Python**: 3.12.12\n",
    "\n",
    "**Note**: Minor version differences are usually fine, but major version mismatches may cause compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "version_check_code",
    "outputId": "09637a7d-0e09-42f9-9474-ebc77b4d8925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PACKAGE VERSIONS\n",
      "============================================================\n",
      "Keras version: 3.12.0\n",
      "Keras-Hub version: 0.23.0\n",
      "TensorFlow version: 2.19.0\n",
      "JAX version: 0.7.2\n",
      "TensorFlow Text version: 2.19.0\n",
      "Python version: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
      "============================================================\n",
      "\n",
      "‚úÖ All packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERSION VERIFICATION\n",
    "# ============================================================================\n",
    "# Display all package versions for reproducibility and troubleshooting\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "try:\n",
    "    import tensorflow_text as tf_text\n",
    "    tf_text_version = tf_text.__version__\n",
    "except ImportError:\n",
    "    tf_text_version = \"Not installed\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PACKAGE VERSIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Keras-Hub version: {keras_hub.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"TensorFlow Text version: {tf_text_version}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ All packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RCE3fdGhDE5"
   },
   "source": [
    "## üîΩ Load Gemma 3 Model\n",
    "\n",
    "Now we'll load the pre-trained Gemma 3 model using KerasHub's convenient `from_preset()` method.\n",
    "\n",
    "### What is a Causal Language Model?\n",
    "\n",
    "A **Causal Language Model (CLM)** predicts the next token based on previous tokens in a sequence. This is the foundation of text generation:\n",
    "\n",
    "```\n",
    "Input:  \"The radiation dose is\"\n",
    "Output: \"measured in Gray (Gy)\"\n",
    "```\n",
    "\n",
    "### Available Gemma 3 Presets\n",
    "\n",
    "KerasHub provides several pre-configured Gemma 3 models:\n",
    "\n",
    "| Preset Name | Size | Context | Type | Memory |\n",
    "|-------------|------|---------|------|--------|\n",
    "| `gemma3_instruct_270m` | 270M | 8K | Text-only | ~0.5 GB |\n",
    "| `gemma3_instruct_1b` | 1B | 32K | Text-only | ~1.5 GB |\n",
    "| `gemma3_instruct_4b` | 4B | 128K | Multimodal | ~6.4 GB |\n",
    "| `gemma3_instruct_12b` | 12B | 128K | Multimodal | ~20 GB |\n",
    "| `gemma3_instruct_27b` | 27B | 128K | Multimodal | ~46 GB |\n",
    "\n",
    "**For this tutorial**, we use `gemma3_instruct_270m` - optimized for instruction following and fine-tuning on consumer CPUs and GPUs.\n",
    "\n",
    "**Browse all models**: [Gemma 3 on Kaggle](https://www.kaggle.com/models/keras/gemma3)\n",
    "\n",
    "### Loading Process\n",
    "\n",
    "The `from_preset()` method will:\n",
    "1. Download the model weights from Kaggle (~1.5 GB)\n",
    "2. Load the tokenizer (262K vocabulary)\n",
    "3. Initialize the model architecture\n",
    "4. Load pre-trained weights\n",
    "\n",
    "**Note**: First-time download may take 2-5 minutes depending on your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vz5zLEyLstfn",
    "outputId": "0fbf58f0-7961-4850-ef66-40ce3b4ae7dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Gemma 3 270m Instruct model...\n",
      "‚è≥ This may take 2-5 minutes on first run...\n",
      "\n",
      "\n",
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "# Load Gemma 3 1B Instruct model from Kaggle\n",
    "# This will download ~1.5 GB on first run (cached for future use)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üì• Loading Gemma 3 270m Instruct model...\")\n",
    "print(\"‚è≥ This may take 2-5 minutes on first run...\\n\")\n",
    "\n",
    "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_270m\")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nl4lvPy5zA26"
   },
   "source": [
    "The `Gemma3CausalLM.from_preset()` method instantiates the model from a preset architecture and weights. In the code above, the string `\"gemma#_xxxxxxx\"` specifies a preset version and parameter size for Gemma. You can find the code strings for Gemma models in their **Model Variation** listings on [Kaggle](https://www.kaggle.com/models/keras/gemma3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_L6A5J-1QgC"
   },
   "source": [
    "## üîç Inference Before Fine-Tuning\n",
    "\n",
    "Before fine-tuning, let's test how Gemma 3 270M responds to different prompt formats. **Gemma 3 270M is highly sensitive to prompt engineering** - the way you structure your prompt significantly impacts response quality.\n",
    "\n",
    "### üéØ Sampling Strategy\n",
    "\n",
    "We use **`keras_hub.samplers.TopPSampler`** ([documentation](https://keras.io/keras_hub/api/samplers/top_p_sampler/)) for controlled text generation:\n",
    "- **Top-P (nucleus) sampling**: Selects from the smallest set of tokens whose cumulative probability exceeds `p`\n",
    "- **Top-K filtering**: Limits selection to the `k` most likely tokens\n",
    "- **Parameters**: `p=0.3, k=5, seed=2` for reproducible, focused outputs\n",
    "\n",
    "### üìã Prompt Template Formats\n",
    "\n",
    "We'll test three approaches:\n",
    "\n",
    "**1. Custom Template (Simple)**\n",
    "```python\n",
    "\"System Instruction:\\n{system_instruction}\\n\\nQuestion:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "```\n",
    "\n",
    "**2. Gemma Official Chat Template**\n",
    "```python\n",
    "\"<start_of_turn>user\\n{content}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "```\n",
    "- `<start_of_turn>user` - Marks user input\n",
    "- `<end_of_turn>` - Signals turn end\n",
    "- `<start_of_turn>model` - Model response starts here\n",
    "\n",
    "Let's see how each performs! üß™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå Test 1: No Chat Template, No System Instruction\n",
    "\n",
    "**Approach**: Simple instruction-response format without system context.\n",
    "\n",
    "**Expected Issue**: Without proper guidance, the model may hallucinate or provide incorrect answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZCqT4zBWQc12",
    "outputId": "5f9de168-951c-4e20-b073-cc57c9fa73ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "What is the double-helix molecule primarily responsible for heredity and the synthesis of proteins?\n",
      "\n",
      "Response:\n",
      "The double-helix molecule primarily responsible for heredity and the synthesis of proteins is a **protein**.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=\"What is the double-helix molecule primarily responsible for heredity and the synthesis of proteins?\",\n",
    "    response=\"\",\n",
    ")\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.3, k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: ‚ùå **WRONG!** The model says \"protein\" instead of \"DNA\". Without system instruction, Gemma 3 270M struggles with basic questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Test 2: Custom Template + System Instruction\n",
    "\n",
    "**Approach**: Add system instruction to guide the model's behavior.\n",
    "\n",
    "**Hypothesis**: System context should improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9njZQ44bQc14",
    "outputId": "edc5b4bc-0b81-4789-f089-0922c87c2b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Instruction:\n",
      "You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\n",
      "\n",
      "Question:\n",
      "What is the double-helix molecule primarily responsible for heredity and the synthesis of proteins?\n",
      "\n",
      "Response:\n",
      "The double-helix molecule primarily responsible for heredity and the synthesis of proteins is the **DNA**.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_INSTRUCTION = \"You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\"\n",
    "instruction_text = \"What is the double-helix molecule primarily responsible for heredity and the synthesis of proteins?\"\n",
    "template = \"System Instruction:\\n{system_instruction}\\n\\nQuestion:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "prompt = template.format(\n",
    "    system_instruction=SYSTEM_INSTRUCTION,\n",
    "    instruction=instruction_text,\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.3, k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: ‚úÖ **CORRECT!** The model now correctly answers \"DNA\". System instruction provides crucial context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Test 3: Gemma Official Chat Template\n",
    "\n",
    "**Approach**: Use Gemma's official chat format with turn delimiters.\n",
    "\n",
    "**Hypothesis**: Official template should work equally well or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lorJMbsusgoo",
    "outputId": "1f853764-54f1-4ae3-8dd9-36a94383307e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.Question:\n",
      "What is the double-helix molecule primarily responsible for heredity and the synthesis of proteins?\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The double-helix molecule primarily responsible for heredity and the synthesis of proteins is **DNA**.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Model-Specific Constants and System Instructions ---\n",
    "# These are the standard delimiters for Gemma instruction-tuned models.\n",
    "START_TURN_USER = \"<start_of_turn>user\\n\"\n",
    "END_TURN = \"<end_of_turn>\\n\"\n",
    "START_TURN_MODEL = \"<start_of_turn>model\\n\"\n",
    "\n",
    "# System/Role instruction is prepended to the user's turn\n",
    "SYSTEM_INSTRUCTION = \"You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\"\n",
    "USER_QUESTION = \"What is the double-helix molecule primarily responsible for heredity and the synthesis of proteins?\"\n",
    "\n",
    "# Step 2a: Create the content of the user turn\n",
    "# We wrap the SYSTEM_INSTRUCTION and the USER_QUESTION together.\n",
    "user_content = f\"{SYSTEM_INSTRUCTION}Question:\\n{USER_QUESTION}\"\n",
    "\n",
    "# Step 2b: Assemble the full prompt with turn delimiters\n",
    "# The model is trained to start generating after <start_of_turn>model\n",
    "prompt = (\n",
    "    f\"{START_TURN_USER}\"  # <start_of_turn>user\\n\n",
    "    f\"{user_content}\\n\"\n",
    "    f\"{END_TURN}\"         # <end_of_turn>\\n\n",
    "    f\"{START_TURN_MODEL}\" # <start_of_turn>model\\n\n",
    ")\n",
    "# --- 3. Configure and Generate ---\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.3, k=5, seed=2)\n",
    "\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: ‚úÖ **CORRECT!** Gemma chat template also produces accurate response. Both custom and official templates work when system instruction is included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Test 4: Custom Template - Looping Issue\n",
    "\n",
    "**Approach**: Test with a different question using custom template.\n",
    "\n",
    "**Watch for**: Potential hallucination or repetitive output patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ddCBWVDDQc15",
    "outputId": "337aef68-fb25-4f3b-80ca-cbfc591c4a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Instruction:\n",
      "You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\n",
      "\n",
      "Question:\n",
      "Who Discovered the Structure of DNA?\n",
      "\n",
      "Response:\n",
      "The discovery of the structure of DNA was made by the geneticist, **Watson and Crick**.\n",
      "\n",
      "Question:\n",
      "What is the purpose of the DNA double helix?\n",
      "\n",
      "Response:\n",
      "The DNA double helix is a structure that allows for the precise replication of genetic information. It provides a stable and organized structure for the proteins that carry the genetic code.\n",
      "\n",
      "Question:\n",
      "What is the function of the DNA double helix?\n",
      "\n",
      "Response:\n",
      "The DNA double helix is a structural framework that provides the stability and organization of the proteins that carry the genetic code. It also helps to maintain the integrity of the DNA.\n",
      "\n",
      "Question:\n",
      "What is the significance of the DNA double helix?\n",
      "\n",
      "Response:\n",
      "The DNA double helix is a fundamental structure that plays a crucial role in the evolution of life. It is essential for the development of all life forms.\n",
      "\n",
      "Question:\n",
      "What is the role of the DNA double helix in the cell?\n",
      "\n",
      "Response:\n",
      "The DNA double helix is a crucial structural component of the cell, providing the structural support and stability of the cell. It\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_INSTRUCTION = \"You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\"\n",
    "instruction_text = \"Who Discovered the Structure of DNA?\"\n",
    "template = \"System Instruction:\\n{system_instruction}\\n\\nQuestion:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "prompt = template.format(\n",
    "    system_instruction=SYSTEM_INSTRUCTION,\n",
    "    instruction=instruction_text,\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.3, k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: ‚ö†Ô∏è **LOOPING!** The model generates repetitive Q&A pairs instead of stopping. Custom template can cause instability with Gemma 3 270M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Test 5: Gemma Chat Template - Stable Output\n",
    "\n",
    "**Approach**: Same question but with Gemma's official chat template.\n",
    "\n",
    "**Hypothesis**: Official template should prevent looping behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZQt71szDQc15",
    "outputId": "6ba00eae-0d6a-4fee-b6e3-eef800936e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>\n",
      "You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.Question:\n",
      "Who Discovered the Structure of DNA?\n",
      "<end_of_turn>\n",
      "<start_of_turn>Response:\n",
      "The discovery of the structure of DNA was made by **Watson and Crick** in 1953.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Model-Specific Constants and System Instructions ---\n",
    "# These are the standard delimiters for Gemma instruction-tuned models.\n",
    "START_TURN_USER = \"<start_of_turn>\\n\"\n",
    "END_TURN = \"<end_of_turn>\\n\"\n",
    "START_TURN_MODEL = \"<start_of_turn>Response:\\n\"\n",
    "\n",
    "# System/Role instruction is prepended to the user's turn\n",
    "SYSTEM_INSTRUCTION = \"You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\"\n",
    "USER_QUESTION = \"Who Discovered the Structure of DNA?\"\n",
    "\n",
    "# Step 2a: Create the content of the user turn\n",
    "# We wrap the SYSTEM_INSTRUCTION and the USER_QUESTION together.\n",
    "user_content = f\"{SYSTEM_INSTRUCTION}Question:\\n{USER_QUESTION}\"\n",
    "\n",
    "# Step 2b: Assemble the full prompt with turn delimiters\n",
    "# The model is trained to start generating after <start_of_turn>model\n",
    "prompt = (\n",
    "    f\"{START_TURN_USER}\"  # <start_of_turn>user\\n\n",
    "    f\"{user_content}\\n\"\n",
    "    f\"{END_TURN}\"         # <end_of_turn>\\n\n",
    "    f\"{START_TURN_MODEL}\" # <start_of_turn>model\\n\n",
    ")\n",
    "# --- 3. Configure and Generate ---\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.3, k=5, seed=2)\n",
    "\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: ‚úÖ **STABLE & CORRECT!** Gemma chat template produces concise, accurate response without looping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBQieduRizZf"
   },
   "source": [
    "### üìä Summary: Prompt Engineering Matters for Gemma 3 270M\n",
    "\n",
    "**Key Findings**:\n",
    "\n",
    "| Template Type | System Instruction | Result | Issue |\n",
    "|---------------|-------------------|--------|-------|\n",
    "| Simple | ‚ùå No | ‚ùå Wrong answer | Hallucination (\"protein\" instead of \"DNA\") |\n",
    "| Custom | ‚úÖ Yes | ‚úÖ Correct | Works but can cause looping |\n",
    "| Gemma Official | ‚úÖ Yes | ‚úÖ Correct | Stable, no looping |\n",
    "\n",
    "**Recommendations**:\n",
    "\n",
    "1. **For Gemma 3 270M**: Use **Gemma official chat template** for best stability\n",
    "2. **Always include system instructions** - critical for accuracy\n",
    "3. **Custom templates work** but may cause repetitive outputs\n",
    "4. **Larger models (1B+)** are less sensitive to prompt format\n",
    "\n",
    "**Theoretical vs. Practical**: While Gemma's official template is theoretically better, **test your own prompts** - results vary by use case. For production with Gemma 3 270M, official template is safer.\n",
    "\n",
    "Now let's see how fine-tuning improves domain-specific knowledge! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt7Nr6a7tItO"
   },
   "source": [
    "## LoRA fine-tuning\n",
    "\n",
    "This section shows you how to do fine-tuning using the Low Rank Adaptation (LoRA) tuning technique. This approach allows you to change the behavior of Gemma models using fewer compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T7xe_jzslv4"
   },
   "source": [
    "### üìä Load Custom Radiobiology Dataset\n",
    "\n",
    "Now we'll load our custom dataset generated from the Radiobiology textbook using the **LLM-DATA-Generator** tool.\n",
    "\n",
    "#### Dataset Overview\n",
    "\n",
    "Our dataset contains:\n",
    "- **832 question-answer pairs** from \"Molecular Radiation Biology\" textbook\n",
    "- **Domain**: Radiobiology, radiation physics, medical applications\n",
    "- **Format**: CSV with columns: `Question`, `Answer`, `Content`\n",
    "\n",
    "#### üéØ Chat Template Strategy for Fine-Tuning\n",
    "\n",
    "**You can fine-tune with any chat template!** In this tutorial, we will **not use any specific chat template** to keep the fine-tuning general and flexible. This allows you to use the fine-tuned model with **any chat template** after training.\n",
    "\n",
    "**We recommend testing different chat templates** during fine-tuning to find what works best for your use case:\n",
    "\n",
    "**Option 1: Custom Template (Used in this tutorial)**\n",
    "```python\n",
    "CUSTOM_TEMPLATE = (\n",
    "    \"System Instruction:\\n{system_instruction}\\n\\n\"\n",
    "    \"Question:\\n{instruction}\\n\\n\"\n",
    "    \"Response:\\n{response}\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Option 2: Gemma Official Chat Template**\n",
    "```python\n",
    "GEMMA_TEMPLATE = (\n",
    "    \"<start_of_turn>user\\n\"\n",
    "    \"{system_instruction}\\n\\n\"\n",
    "    \"{instruction}<end_of_turn>\\n\"\n",
    "    \"<start_of_turn>model\\n\"\n",
    "    \"{response}<end_of_turn>\"\n",
    ")\n",
    "```\n",
    "\n",
    "**üí° Tip**: Experiment with both templates during fine-tuning to see which produces better results for your specific domain!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_Bi-WZcQc17",
    "outputId": "c16dfd71-1d9f-4738-b3a8-4d6ce1da25dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example of the First Template-Based Prompt (Input) ---\n",
      "\n",
      "Describe the process of DNA damage repair following ionizing radiation exposure, focusing on at least three distinct mechanisms.\n",
      "\n",
      "--- Example of the Corresponding Response (Target) ---\n",
      "\n",
      "Following ionizing radiation exposure, DNA undergoes significant damage, primarily in the form of single-strand breaks (SSBs) and double-strand breaks (DSBs). Cellular response involves a complex network of DNA repair pathways to mitigate this damage. One key mechanism is Non-Homologous End Joining (NHEJ), which directly ligates broken DNA ends without requiring a homologous template. This process is error-prone, often resulting in small insertions or deletions. Another crucial pathway is Homologous Recombination (HR), which utilizes the undamaged sister chromatid as a template to accurately repair DSBs. HR requires stalled replication forks and proceeds through multiple steps including strand invasion and resolution. Finally, Base Excision Repair (BER) addresses damage to individual DNA bases caused by radiation-induced oxidation or alkylation. BER involves recognition of damaged bases by glycosylases, followed by removal of the damaged base and subsequent replacement with an appropriate nucleotide using DNA polymerase. These three mechanisms ‚Äì NHEJ, HR, and BER ‚Äì represent critical components of the cellular response to ionizing radiation damage, each contributing uniquely to maintaining genomic stability. Reference: Molecular Radiation Biology, Section 3.2 ‚Äì DNA Damage Repair Mechanisms.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "prompts = df[\"Question\"].tolist()\n",
    "responses = df[\"Answer\"].tolist()\n",
    "SYSTEM_INSTRUCTION = \"You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\\n\"\n",
    "\n",
    "updated_prompts = [SYSTEM_INSTRUCTION + p for p in prompts]\n",
    "data = {\n",
    "    \"prompts\": prompts,\n",
    "    \"responses\": responses\n",
    "}\n",
    "\n",
    "# --- 5. Verify Output ---\n",
    "print(\"\\n--- Example of the First Template-Based Prompt (Input) ---\\n\")\n",
    "print(data[\"prompts\"][5])\n",
    "print(\"\\n--- Example of the Corresponding Response (Target) ---\\n\")\n",
    "print(data[\"responses\"][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBLW5hiGj31i"
   },
   "source": [
    "### ‚öôÔ∏è Configure LoRA Fine-Tuning\n",
    "\n",
    "Now we'll activate LoRA on our Gemma model using the `enable_lora()` method.\n",
    "\n",
    "#### Understanding LoRA Rank\n",
    "\n",
    "The **rank (r)** parameter is crucial - it determines the dimensionality of the low-rank matrices:\n",
    "\n",
    "**Mathematical Perspective**:\n",
    "- Original weight matrix: **W** (d √ó d) - frozen\n",
    "- LoRA matrices: **A** (d √ó r) and **B** (r √ó d) - trainable\n",
    "- Updated weight: **W' = W + B √ó A**\n",
    "\n",
    "**Practical Impact**:\n",
    "\n",
    "The choice of rank depends on your specific use case. You can use various ranks like 4, 8, 16, 32, 64, 128, etc.\n",
    "\n",
    "**General Guidelines**:\n",
    "- **Lower ranks (4, 8)**: Faster training, less memory, good for simple tasks\n",
    "- **Medium ranks (16, 32)**: Balanced performance, suitable for most domain adaptation\n",
    "- **Higher ranks (64, 128+)**: More expressive, better for complex tasks, requires more resources\n",
    "\n",
    "#### Our Choice: Rank = 8\n",
    "\n",
    "For this tutorial, we use **rank=8** because:\n",
    "- ‚úÖ Good balance between performance and efficiency\n",
    "- ‚úÖ Works well on RTX 4060 8GB\n",
    "- ‚úÖ Sufficient for domain-specific Q&A adaptation\n",
    "- ‚úÖ Recommended starting point for most tasks\n",
    "\n",
    "#### Best Practices\n",
    "\n",
    "1. **Start with rank=8**: Good default for most tasks\n",
    "2. **Evaluate**: Measure performance on validation set\n",
    "3. **Adjust based on results**:\n",
    "   - If underfitting ‚Üí increase rank (16, 32, 64)\n",
    "   - If overfitting or memory issues ‚Üí decrease rank (4)\n",
    "4. **Consider dataset complexity**: More complex/diverse data may benefit from higher ranks\n",
    "5. **Monitor GPU memory**: Adjust rank if you encounter OOM errors\n",
    "\n",
    "**Reference**: [LoRA Paper - Section 4.2](https://arxiv.org/abs/2106.09685)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lora_explanation"
   },
   "source": [
    "### üéØ Enable LoRA - The Magic Moment!\n",
    "\n",
    "Time to flip the switch and activate LoRA! This is where we tell the model: \"Hey buddy, we're only updating a tiny fraction of your weights, so relax!\" üòé\n",
    "\n",
    "#### üéõÔ∏è The `rank=8` Choice - Goldilocks Would Approve! üêª\n",
    "\n",
    "We're setting `rank=8` as our sweet spot:\n",
    "- **Too small** (rank=4): Might underfit - like trying to paint the Mona Lisa with only 4 colors üé®\n",
    "- **Too big** (rank=64): Overkill - like using a flamethrower to light a birthday candle üî•üéÇ\n",
    "- **Just right** (rank=8): Perfect balance of expressiveness and efficiency! ‚ú®\n",
    "\n",
    "#### üçî The Burger Analogy\n",
    "\n",
    "Think of it like ordering a burger:\n",
    "- **rank=8** is like customizing the patty (the main thing we care about)\n",
    "- **Other parameters** are the chef's recommended toppings (we trust the defaults!)\n",
    "\n",
    "#### üéõÔ∏è Other LoRA Parameters (Kept at Defaults)\n",
    "\n",
    "The `enable_lora()` method has other parameters we're keeping at their default values:\n",
    "\n",
    "| Parameter | Default | What It Does |\n",
    "|-----------|---------|-------------|\n",
    "| `rank` | **8** (we set this!) | Size of the LoRA factorization matrices |\n",
    "| `target_layer_names` | `None` (auto) | Which layers get LoRA (defaults to query & value layers) |\n",
    "\n",
    "When `target_layer_names=None`, KerasHub automatically targets:\n",
    "- `\"query_dense\"`, `\"value_dense\"`, `\"query\"`, `\"value\"`\n",
    "\n",
    "These are the attention mechanism's query and value projection layers - the most important parts for fine-tuning! üéØ\n",
    "\n",
    "#### üìö Want to Dive Deeper?\n",
    "\n",
    "Check out the **source code** on GitHub to see all the magic happening under the hood:\n",
    "\n",
    "üîó [KerasHub `enable_lora()` source code](https://github.com/keras-team/keras-hub/blob/master/keras_hub/src/models/backbone.py#L191)\n",
    "\n",
    "**Method signature:**\n",
    "```python\n",
    "def enable_lora(self, rank, target_layer_names=None):\n",
    "    # rank: The rank of the LoRA factorization (we're using 8)\n",
    "    # target_layer_names: List of layer names to apply LoRA to\n",
    "    #   (None = uses default: [\"query_dense\", \"value_dense\", \"query\", \"value\"])\n",
    "```\n",
    "\n",
    "Now let's activate it! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCucu6oHz53G"
   },
   "outputs": [],
   "source": [
    "# Enable LoRA with rank=8\n",
    "gemma_lm.backbone.enable_lora(rank=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlMLp_NVbRoQ"
   },
   "source": [
    "Check the model summary after setting the LoRA rank. Notice that enabling LoRA reduces the number of trainable parameters significantly compared to the total number of parameters in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "KqYyS0gm6pNy",
    "outputId": "289f58a1-ff86-4999-f669-a246eb34daa6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                                                  </span>‚îÉ<span style=\"font-weight: bold\">                                   Config </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            ‚îÇ                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            ‚îÇ                      Vocab size: \u001b[38;5;34m262,144\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                  </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape              </span>‚îÉ<span style=\"font-weight: bold\">         Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to               </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                          ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                          ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ gemma3_backbone               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)         ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">268,632,704</span> ‚îÇ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              ‚îÇ                           ‚îÇ                 ‚îÇ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ token_embedding               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">167,772,160</span> ‚îÇ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ -                          ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ -                          ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ gemma3_backbone               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)         ‚îÇ     \u001b[38;5;34m268,632,704\u001b[0m ‚îÇ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              ‚îÇ                           ‚îÇ                 ‚îÇ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ token_embedding               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      ‚îÇ     \u001b[38;5;34m167,772,160\u001b[0m ‚îÇ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,632,704</span> (1.00 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m268,632,704\u001b[0m (1.00 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">534,528</span> (2.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m534,528\u001b[0m (2.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,098,176</span> (1022.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m268,098,176\u001b[0m (1022.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQQ47kcdpbZ9"
   },
   "source": [
    "### ‚öôÔ∏è Configure Training Parameters - The Recipe for Success! üç≥\n",
    "\n",
    "Now comes the fun part - setting up our training configuration! Think of this as preparing your kitchen before cooking a gourmet meal. üë®‚Äçüç≥\n",
    "\n",
    "We'll configure:\n",
    "1. **Sequence Length**: How much text the model processes at once\n",
    "2. **Optimizer**: The \"learning algorithm\" that updates our LoRA weights\n",
    "3. **Loss Function**: How we measure \"wrongness\" during training\n",
    "4. **Metrics**: How we track progress\n",
    "\n",
    "Let's break down each parameter with some fun analogies! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sequence_length_explanation"
   },
   "source": [
    "#### üìè Sequence Length - The Attention Span Setting\n",
    "\n",
    "**Setting:** `1024 tokens` (~750 words)\n",
    "\n",
    "üß† **What it does:** Limits how much text the model processes at once. Think of it as the model's \"attention span\" - how many words it can keep in its head while thinking! ü§î\n",
    "\n",
    "üéØ **Why 1024? The Goldilocks Zone:**\n",
    "- **Too short (256):** Model can't see enough context - like reading a book one sentence at a time! üìñ\n",
    "- **Too long (4096):** Your GPU goes \"NOPE!\" üí• (Out of memory error incoming!)\n",
    "- **Just right (1024):** Fits our 8GB GPU like a glove! üß§ Perfect balance of context and memory.\n",
    "\n",
    "üîß **How to change:**\n",
    "```python\n",
    "gemma_lm.preprocessor.sequence_length = YOUR_VALUE\n",
    "```\n",
    "- Try `512` for faster training (less memory)\n",
    "- Try `2048` for more context (needs more VRAM - 16GB+ recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9sBNH8SAjgB"
   },
   "outputs": [],
   "source": [
    "# Set sequence length to 1024 tokens\n",
    "gemma_lm.preprocessor.sequence_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optimizer_explanation"
   },
   "source": [
    "#### üöÄ Optimizer - The Learning Engine (AdamW)\n",
    "\n",
    "Think of the optimizer as a **GPS for finding the best model weights**. AdamW is like Google Maps with traffic updates - smart, efficient, and knows when to slow down! üó∫Ô∏è\n",
    "\n",
    "**We're using 2 out of 16 available parameters!** (Keeping it simple, folks! üòé)\n",
    "\n",
    "##### 1Ô∏è‚É£ `learning_rate = 5e-5` (0.00005)\n",
    "\n",
    "üß† **What:** How big of a step we take when updating weights\n",
    "\n",
    "üéØ **Why this value? The Espresso Analogy:** ‚òï\n",
    "- **Too high (1e-3):** Model goes crazy, overshoots the target like a caffeinated squirrel! üêøÔ∏èüé¢\n",
    "- **Too low (1e-6):** Training takes FOREVER, like watching paint dry in slow motion üêå\n",
    "- **Just right (5e-5):** Steady progress, like a perfectly brewed espresso - smooth and effective! ‚ú®\n",
    "\n",
    "üîß **How to change:**\n",
    "- Try `1e-4` for faster training (riskier, might overshoot)\n",
    "- Try `1e-5` for safer training (slower, more stable)\n",
    "\n",
    "##### 2Ô∏è‚É£ `weight_decay = 0.01`\n",
    "\n",
    "üß† **What:** Regularization that prevents overfitting (memorizing training data)\n",
    "\n",
    "üéØ **Why:** Like telling a student **\"understand the concept, don't just memorize!\"** üìö\n",
    "- Keeps weights small and prevents the model from being too confident\n",
    "- `0.01` is the \"Goldilocks zone\" - not too strict, not too loose\n",
    "- Without it, the model might ace the training data but fail on new questions! üòÖ\n",
    "\n",
    "üîß **How to change:**\n",
    "- Try `0.001` for less regularization (if underfitting)\n",
    "- Try `0.1` for more regularization (if overfitting)\n",
    "\n",
    "##### üìö Want to See ALL 16 AdamW Parameters?\n",
    "\n",
    "Check out the **official Keras documentation:**\n",
    "\n",
    "üîó [Keras AdamW Optimizer Documentation](https://keras.io/api/optimizers/adamw/)\n",
    "\n",
    "**Other useful parameters you might want to explore:**\n",
    "\n",
    "| Parameter | Default | What It Does |\n",
    "|-----------|---------|-------------|\n",
    "| `beta_1` | 0.9 | Exponential decay rate for 1st moment estimates |\n",
    "| `beta_2` | 0.999 | Exponential decay rate for 2nd moment estimates |\n",
    "| `epsilon` | 1e-7 | Small constant for numerical stability |\n",
    "| `amsgrad` | False | Whether to use AMSGrad variant |\n",
    "| `clipnorm` | None | Gradient clipping by norm |\n",
    "| `clipvalue` | None | Gradient clipping by value |\n",
    "| `global_clipnorm` | None | Global gradient clipping |\n",
    "| `use_ema` | False | Exponential moving average of weights |\n",
    "| `ema_momentum` | 0.99 | Momentum for EMA |\n",
    "| `gradient_accumulation_steps` | None | For gradient accumulation |\n",
    "\n",
    "...and more! Check the docs for the full list.\n",
    "\n",
    "üéì **Fun Fact:** The \"W\" in AdamW stands for \"Weight Decay\" - it's Adam's cooler, more disciplined cousin! üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimizer_code"
   },
   "outputs": [],
   "source": [
    "# Create AdamW optimizer\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Exclude bias and scale parameters from weight decay (best practice)\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exclude_weight_decay_explanation"
   },
   "source": [
    "#### üéØ Why Exclude Bias & Scale from Weight Decay?\n",
    "\n",
    "**The Seasoning Analogy:** üßÇ\n",
    "\n",
    "Bias and scale parameters are like the **seasoning** in our model. We don't want to regularize them because:\n",
    "- They're already small and well-behaved üòá\n",
    "- Decaying them can hurt performance (like under-salting your food! üßÇ)\n",
    "- They serve a different purpose than the main weights\n",
    "\n",
    "This is a **best practice from the transformer research community** - trust the experts! üéì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_explanation"
   },
   "source": [
    "#### üéØ Compile the Model - Putting It All Together!\n",
    "\n",
    "This is like **assembling your IKEA furniture** - all pieces come together! üõ†Ô∏è\n",
    "\n",
    "##### üìä Components:\n",
    "\n",
    "##### 1Ô∏è‚É£ Loss Function: `SparseCategoricalCrossentropy`\n",
    "\n",
    "üß† **What:** Measures how \"wrong\" the model's predictions are\n",
    "\n",
    "üéØ **Why it's perfect for language modeling:**\n",
    "- **\"Sparse\"** = targets are integers (token IDs), not one-hot vectors (saves memory!)\n",
    "- **\"Categorical\"** = choosing from 262,144 possible tokens (Gemma's vocabulary)\n",
    "- **\"Crossentropy\"** = the math that makes it work (trust us, it's good! ü§ì)\n",
    "\n",
    "üîß **`from_logits=True`:** Model outputs raw scores, not probabilities (more numerically stable)\n",
    "\n",
    "##### 2Ô∏è‚É£ Metrics: `SparseCategoricalAccuracy`\n",
    "\n",
    "üß† **What:** Percentage of tokens predicted correctly\n",
    "\n",
    "üéØ **Why it's useful:**\n",
    "- Easy to understand - \"Did we guess the right word?\" ‚úÖ\n",
    "- `0.48 accuracy` = 48% of tokens correct (not bad for 262K choices!)\n",
    "- Helps us track if training is actually working üìà\n",
    "\n",
    "##### üéì Fun Analogy:\n",
    "- **Loss** is like your GPS saying *\"you're 5 miles off course\"* üó∫Ô∏è\n",
    "- **Accuracy** is like saying *\"you got 8 out of 10 turns right!\"* üöó\n",
    "\n",
    "Both are useful, but they tell you different things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile_code"
   },
   "outputs": [],
   "source": [
    "# Compile the model with loss, optimizer, and metrics\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx3m8f1dB7nk"
   },
   "source": [
    "#### Mixed precision fine-tuning on NVIDIA GPUs\n",
    "\n",
    "Full precision is recommended for fine-tuning. When fine-tuning on NVIDIA GPUs, you can use mixed precision (`keras.mixed_precision.set_global_policy('mixed_bfloat16')`) to speed up training with minimal effect on training quality.\n",
    "\n",
    "**Fun fact**: Even though this is labeled for GPUs, your CPU can accept it too! üòÑ Mixed precision works on CPUs, though the speedup benefits are primarily seen on modern GPUs with tensor cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0lHxEDX03gp"
   },
   "outputs": [],
   "source": [
    "keras.mixed_precision.set_global_policy('mixed_bfloat16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA0ozGC66tk1"
   },
   "source": [
    "### üèãÔ∏è Run the Fine-Tuning Process - Training Time!\n",
    "\n",
    "Now for the main event - actually training our model! This is where the magic happens. ‚ú®\n",
    "\n",
    "#### Understanding the `fit()` Parameters\n",
    "\n",
    "We're using `gemma_lm.fit(data, epochs=3, batch_size=1)` - let's break down what each parameter means:\n",
    "\n",
    "**1Ô∏è‚É£ epochs = 3** üîÑ\n",
    "- **What it means**: How many times the model sees the ENTIRE dataset\n",
    "- **The analogy**: Like reading a textbook cover-to-cover\n",
    "  - 1 epoch = Read it once üìñ\n",
    "  - 3 epochs = Read it three times üìöüìöüìö\n",
    "  - 10 epochs = You're basically memorizing it! üß†\n",
    "- **Why 3?** For LoRA fine-tuning on domain-specific data, 3 epochs provides a good balance!\n",
    "  - More epochs = Better performance BUT risk of overfitting (memorizing instead of learning)\n",
    "  - Our 832 Q&A pairs are high-quality, so 3 epochs gives solid convergence! üéØ\n",
    "  - You'll see loss decrease across epochs: Epoch 1 (0.67) ‚Üí Epoch 2 (0.63) ‚Üí Epoch 3 (0.62)\n",
    "- **üîß How to change**: Try `epochs=1` for faster training, or `epochs=5` for potentially better results (watch for overfitting!)\n",
    "\n",
    "**2Ô∏è‚É£ batch_size = 1** üì¶\n",
    "- **What it means**: How many examples the model processes before updating weights\n",
    "- **The analogy**: Like grading homework\n",
    "  - batch_size=1: Grade one paper, update your answer key, repeat üéì\n",
    "  - batch_size=32: Grade 32 papers, THEN update your answer key üìö\n",
    "- **Why 1?** Memory constraints on our 8GB GPU! üéÆ\n",
    "  - Larger batches = Faster training BUT need more VRAM\n",
    "  - batch_size=1 = Slower but fits comfortably in 8GB\n",
    "  - Also called \"online learning\" or \"stochastic gradient descent\" (fancy! üé©)\n",
    "- **üîß How to change**:\n",
    "  - If you have 16GB+ VRAM: Try `batch_size=4` or `batch_size=8` (2-4x faster!)\n",
    "  - If you get OOM errors: Keep it at 1 (or reduce sequence_length)\n",
    "\n",
    "**‚è±Ô∏è Training Time Expectations**:\n",
    "- **RTX 4060 8GB**: ~15-30 minutes for 832 examples √ó 3 epochs (~2542s per epoch)\n",
    "- **RTX 3090 24GB**: ~10-15 minutes (with larger batch_size)\n",
    "- **CPU only**: Don't even think about it! ‚òï‚òï‚òï (hours!)\n",
    "\n",
    "**üìä What to Watch During Training**:\n",
    "- **Loss**: Should decrease across epochs (0.67 ‚Üí 0.63 ‚Üí 0.62... lower is better!)\n",
    "- **Accuracy**: Should increase across epochs (0.46 ‚Üí 0.47 ‚Üí 0.48... higher is better!)\n",
    "- **GPU Memory**: Should stay under 8GB (check with `nvidia-smi`)\n",
    "\n",
    "**üéì Pro Tip**: Grab a coffee ‚òï and watch the progress bar. With 3 epochs, you have time for a nice break! üòå\n",
    "\n",
    "Let's start training! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Peq7TnLtHse",
    "outputId": "37ac1856-dceb-47b6-aea2-0c9a99ed5998"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 15:19:13.392767: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2542s\u001b[0m 3s/step - loss: 0.6714 - sparse_categorical_accuracy: 0.4577\n",
      "Epoch 2/3\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2538s\u001b[0m 3s/step - loss: 0.6310 - sparse_categorical_accuracy: 0.4729\n",
      "Epoch 3/3\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2532s\u001b[0m 3s/step - loss: 0.6178 - sparse_categorical_accuracy: 0.4801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7528a4227cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_lm.fit(data, epochs=3, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yd-1cNw1dTn"
   },
   "source": [
    "## üéâ Inference After Fine-Tuning - The Moment of Truth!\n",
    "\n",
    "Drumroll please... ü•Å Let's see how our fine-tuned model performs on the EXACT same question!\n",
    "\n",
    "**Remember the before**: \"Marie and Pierre Curie discovered radioactivity.\" (Incomplete and inaccurate)\n",
    "\n",
    "**Now watch the magic happen...** ‚ú®\n",
    "\n",
    "We'll test with **two different chat templates** to compare results:\n",
    "1. **Gemma Official Chat Template** - Using `<start_of_turn>` delimiters\n",
    "2. **Custom Template** - Using our simple format from training\n",
    "\n",
    "### Test 1: Gemma Official Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xDecCzZQc2B",
    "outputId": "a72de2c9-88f5-4b07-c8e4-682b42d24a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>\n",
      "You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.Question:\n",
      "Describe the process of DNA repair in bacteria, outlining at least three distinct mechanisms and explaining the specific damage each addresses.\n",
      "<end_of_turn>\n",
      "<start_of_turn>Response:\n",
      "DNA repair is a crucial process in bacterial cells that protects the integrity of the genetic code by repairing damaged DNA strands. It primarily occurs through two main mechanisms:\n",
      "\n",
      "1. **Base excision and circular DNA strand breakage:** This mechanism involves the removal of a portion of the double-strand overhangs (breaks) on the DNA backbone. This process is triggered by mutations that cause mutations in the bacterial genome. The repair process is often triggered by the addition of a DNA repair enzyme, such as BRCA1 or BRCA2, which specifically targets the damaged base pairs.\n",
      "\n",
      "2. **Circular DNA strand repair:** This mechanism involves the removal of a portion of the circular DNA strand, often referred to as the \"replication problem.\" This occurs when a mutation occurs that causes the circular DNA to break, leading to a new, identical strand. The repair process is triggered by the addition of a DNA repair enzyme, such as BRCA1 or BRCA2, which specifically targets the circular DNA.\n",
      "\n",
      "3. **Non-homologous strand breakage:** This mechanism involves the removal of a portion of the non-homologous strand (the strand that is not part of the original DNA sequence). This occurs when a mutation occurs that causes the non-homologous strand to break, leading to a new, identical strand. The repair process is triggered by the addition of a DNA repair enzyme, such as BRCA1 or BRCA2, which specifically targets the non-homologous strand.\n",
      "\n",
      "In summary, DNA repair utilizes a combination of enzymes and mechanisms to correct errors in DNA replication and repair the double-strand break that occurs during replication.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Model-Specific Constants and System Instructions ---\n",
    "# These are the standard delimiters for Gemma instruction-tuned models.\n",
    "START_TURN_USER = \"<start_of_turn>\\n\"\n",
    "END_TURN = \"<end_of_turn>\\n\"\n",
    "START_TURN_MODEL = \"<start_of_turn>Response:\\n\"\n",
    "\n",
    "# System/Role instruction is prepended to the user's turn\n",
    "SYSTEM_INSTRUCTION = \"You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\"\n",
    "USER_QUESTION = \"Describe the process of DNA repair in bacteria, outlining at least three distinct mechanisms and explaining the specific damage each addresses.\"\n",
    "\n",
    "# Step 2a: Create the content of the user turn\n",
    "# We wrap the SYSTEM_INSTRUCTION and the USER_QUESTION together.\n",
    "user_content = f\"{SYSTEM_INSTRUCTION}Question:\\n{USER_QUESTION}\"\n",
    "\n",
    "# Step 2b: Assemble the full prompt with turn delimiters\n",
    "# The model is trained to start generating after <start_of_turn>model\n",
    "prompt = (\n",
    "    f\"{START_TURN_USER}\"  # <start_of_turn>user\\n\n",
    "    f\"{user_content}\\n\"\n",
    "    f\"{END_TURN}\"         # <end_of_turn>\\n\n",
    "    f\"{START_TURN_MODEL}\" # <start_of_turn>model\\n\n",
    ")\n",
    "# --- 3. Configure and Generate ---\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.1, k=5, seed=2)\n",
    "\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Custom Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMxiwuJqU-yr",
    "outputId": "9cc63453-a9bc-4af9-b6a8-b9318c0fd63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Instruction:\n",
      "You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\n",
      "\n",
      "Question:\n",
      "Describe the process of DNA repair in bacteria, outlining at least three distinct mechanisms and explaining the specific damage each addresses.\n",
      "\n",
      "Response:\n",
      "DNA repair is a critical process in bacterial cells that protects the integrity of the genetic code. It occurs through a series of enzymatic reactions that target and neutralize damaged DNA strands. The primary mechanisms involved are:\n",
      "\n",
      "1.  **Base excision and repair:** This process involves removing damaged bases (like thymine or cytosine) from the DNA strand. This is achieved through enzymes like DNA polymerase, which can effectively repair double-strand breaks (DSBs) caused by mutations.\n",
      "2.  **Mismatch repair:** This mechanism targets mismatched bases (e.g., guanine or lysine) that are not properly repaired by DNA polymerase. It utilizes enzymes like DNA polymerase and DNA polymerase I to correct these errors.\n",
      "3.  **Base excision and repair:** This process involves removing damaged bases from the DNA strand, typically through enzymes like DNA polymerase and DNA polymerase I.\n",
      "\n",
      "The specific damage caused by these mechanisms is typically caused by mutations that result in insertions or deletions of base pairs within the DNA sequence. This can lead to mutations that can be passed down through generations, resulting in mutations that can be passed on to offspring.\n",
      "\n",
      "Reference: [MolecularRadiationBiology, Section 10.1.1 ‚Äì DNA Repair Mechanisms]\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_INSTRUCTION = \"You are a concise and expert assistant in Radiobiology. Provide accurate, clear, and relevant answers.\"\n",
    "instruction_text = \"Describe the process of DNA repair in bacteria, outlining at least three distinct mechanisms and explaining the specific damage each addresses.\"\n",
    "template = \"System Instruction:\\n{system_instruction}\\n\\nQuestion:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "prompt = template.format(\n",
    "    system_instruction=SYSTEM_INSTRUCTION,\n",
    "    instruction=instruction_text,\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "sampler = keras_hub.samplers.TopPSampler(p=0.1, k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Comparison: Which Template Performed Better?\n",
    "\n",
    "**Observation**: While I'm not a specialist to definitively say which result is \"good enough,\" there's an interesting difference:\n",
    "\n",
    "| Template | Result Quality | Key Observation |\n",
    "|----------|---------------|------------------|\n",
    "| **Gemma Official** | ‚úÖ Detailed (but not necessarily accurate) | Provides mechanisms but no source reference |\n",
    "| **Custom Template** | ‚úÖ Detailed (but not necessarily accurate) + **Reference** | Includes `Reference: [MolecularRadiationBiology, Section 10.1.1]` |\n",
    "\n",
    "**‚ö†Ô∏è Important Note**: Both responses are **detailed but not necessarily accurate** from a scientific standpoint. The model learned patterns from the training data, but accuracy requires domain expert validation.\n",
    "\n",
    "**üí° Key Insight**: The **Custom Template** gave us the **reference citation** at the end, which suggests it's **more closely converged to the training data format**. Our training data included references, and the custom template successfully learned to reproduce this pattern!\n",
    "\n",
    "This demonstrates that:\n",
    "- ‚úÖ The model **learned the domain knowledge** (radiobiology concepts)\n",
    "- ‚úÖ The model **learned the output format** (including references)\n",
    "- ‚úÖ Custom templates can preserve training data patterns better\n",
    "- ‚ö†Ô∏è **Detailed ‚â† Accurate** - Always validate outputs with domain experts!\n",
    "\n",
    "**Recommendation**: If your training data has specific formatting (like references), using a **custom template that matches your training format** may yield better results! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Your Fine-Tuned Model\n",
    "\n",
    "Now that we've fine-tuned our model, let's save it so we can use it later! The `save_to_preset()` method saves everything you need:\n",
    "\n",
    "**What gets saved:**\n",
    "- ‚úÖ **Model weights** (base model + LoRA adapters merged)\n",
    "- ‚úÖ **Tokenizer** (vocabulary and configuration)\n",
    "- ‚úÖ **Preprocessor** (text processing settings)\n",
    "- ‚úÖ **Configuration files** (all model settings)\n",
    "\n",
    "**Why save as a preset?**\n",
    "- üì¶ **Self-contained**: Everything in one directory\n",
    "- üöÄ **Easy to share**: Upload to Kaggle Models or Hugging Face\n",
    "- üîÑ **Easy to reload**: One line of code to load later\n",
    "\n",
    "Let's save our radiobiology expert model! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dD7QfSRjQc2B"
   },
   "outputs": [],
   "source": [
    "# Save the complete model as a preset\n",
    "preset_dir = \"./radiobiology_gemma3_model\"\n",
    "gemma_lm.save_to_preset(preset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Load Your Fine-Tuned Model Later\n",
    "\n",
    "Want to use your fine-tuned model in a new session? Just load it from the preset directory!\n",
    "\n",
    "**This is useful when:**\n",
    "- üîÅ **Restarting your notebook** - No need to fine-tune again!\n",
    "- üöÄ **Deploying to production** - Load the model in your application\n",
    "- ü§ù **Sharing with others** - They can load your fine-tuned model easily\n",
    "- üíª **Moving to a different machine** - Just copy the directory and load\n",
    "\n",
    "**One line to load everything:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA1PETSVQc2B"
   },
   "outputs": [],
   "source": [
    "# Load the complete fine-tuned model from the preset directory\n",
    "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"./radiobiology_gemma3_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8kFG12l0mVe"
   },
   "source": [
    "## üîß Improving Fine-Tune Results - Level Up Your Model!\n",
    "\n",
    "Our results are already impressive, but there's always room for improvement! Think of this as upgrading from a good espresso to an *exceptional* espresso. ‚òï‚ú®\n",
    "\n",
    "### üìä Current Configuration Summary\n",
    "\n",
    "Here's what we used in this tutorial:\n",
    "\n",
    "| Parameter | Current Value | Purpose |\n",
    "|-----------|---------------|----------|\n",
    "| **Model** | `gemma3_instruct_270m` | Base model (270M parameters) |\n",
    "| **LoRA Rank** | `rank=8` | Adapter expressiveness |\n",
    "| **Epochs** | `epochs=3` | Training passes through data |\n",
    "| **Batch Size** | `batch_size=1` | Examples per weight update |\n",
    "| **Learning Rate** | `5e-5` (0.00005) | Step size for weight updates |\n",
    "| **Weight Decay** | `0.01` | Regularization strength |\n",
    "| **Dataset Size** | 832 Q&A pairs | Training examples |\n",
    "| **Sequence Length** | 512 tokens | Max input/output length |\n",
    "\n",
    "### üéØ Tuning Strategies (From Easiest to Most Advanced)\n",
    "\n",
    "**1Ô∏è‚É£ Increase Training Epochs** üîÑ\n",
    "- **Current**: `epochs=3` (three passes through the data)\n",
    "- **Try**: `epochs=5` or `epochs=7` for better convergence\n",
    "- **Impact**: Model sees examples multiple times, learns patterns deeper\n",
    "- **‚ö†Ô∏è Watch out**: Too many epochs (10+) can cause overfitting (memorization)\n",
    "- **Expected**: Loss should continue decreasing (0.67 ‚Üí 0.63 ‚Üí 0.62 ‚Üí 0.60...)\n",
    "- **How to change**: `gemma_lm.fit(data, epochs=5, batch_size=1)`\n",
    "\n",
    "**2Ô∏è‚É£ Increase LoRA Rank** üìä\n",
    "- **Current**: `rank=8` (good balance for 270M model)\n",
    "- **Try**: `rank=16` or `rank=32` for more capacity\n",
    "- **Impact**: More expressive adapters, can learn more complex patterns\n",
    "- **‚ö†Ô∏è Watch out**: Higher rank = more memory usage and slower training\n",
    "  - `rank=16`: ~1.5x memory, ~1.3x slower\n",
    "  - `rank=32`: ~2x memory, ~1.6x slower\n",
    "- **How to change**: `gemma_lm.backbone.enable_lora(rank=16)`\n",
    "\n",
    "**3Ô∏è‚É£ Adjust Learning Rate** üéõÔ∏è\n",
    "- **Current**: `learning_rate=5e-5` (0.00005)\n",
    "- **Try higher**: `1e-4` (0.0001) for faster convergence\n",
    "  - ‚úÖ Pros: Faster training, reaches good loss quicker\n",
    "  - ‚ö†Ô∏è Cons: Might overshoot optimal weights, less stable\n",
    "- **Try lower**: `1e-5` (0.00001) for more stable training\n",
    "  - ‚úÖ Pros: More stable, better final convergence\n",
    "  - ‚ö†Ô∏è Cons: Slower training, needs more epochs\n",
    "- **How to change**: `optimizer = keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=0.01)`\n",
    "\n",
    "**4Ô∏è‚É£ Tune Weight Decay (Regularization)** üõ°Ô∏è\n",
    "- **Current**: `weight_decay=0.01`\n",
    "- **If overfitting** (training loss ‚â™ validation loss): Increase to `0.05` or `0.1`\n",
    "- **If underfitting** (both losses high): Decrease to `0.001` or `0.005`\n",
    "- **How to change**: `optimizer = keras.optimizers.AdamW(learning_rate=5e-5, weight_decay=0.05)`\n",
    "\n",
    "**5Ô∏è‚É£ Increase Batch Size** üì¶\n",
    "- **Current**: `batch_size=1` (fits in 8GB VRAM)\n",
    "- **If you have 16GB+ VRAM**: Try `batch_size=4` or `batch_size=8`\n",
    "- **Impact**: \n",
    "  - ‚úÖ 2-4x faster training\n",
    "  - ‚úÖ More stable gradient updates\n",
    "  - ‚ö†Ô∏è Requires more VRAM\n",
    "- **How to change**: `gemma_lm.fit(data, epochs=3, batch_size=4)`\n",
    "\n",
    "**6Ô∏è‚É£ Expand Your Dataset** üìö\n",
    "- **Current**: 832 Q&A pairs from one textbook\n",
    "- **Try**: Add more radiobiology textbooks, research papers, or domain-specific content\n",
    "- **Impact**: Broader knowledge, better generalization\n",
    "- **üí° Tip**: Use the LLM-DATA-Generator tool to create more training data!\n",
    "- **Target**: 2000-5000 examples for robust domain adaptation\n",
    "\n",
    "**7Ô∏è‚É£ Increase Sequence Length** üìè\n",
    "- **Current**: `sequence_length=1024` tokens\n",
    "- **Try**: `sequence_length=2048` for longer contexts (if you have more VRAM)\n",
    "- **Impact**: Can handle longer questions and more detailed answers\n",
    "- **‚ö†Ô∏è Watch out**: Significantly increases memory usage (quadratic with length!)\n",
    "- **How to change**: Modify in the data preparation step\n",
    "\n",
    "### üéØ Recommended Improvement Path\n",
    "\n",
    "**For Better Results (Easy):**\n",
    "1. ‚úÖ Increase epochs to 5-7\n",
    "2. ‚úÖ Increase LoRA rank to 16\n",
    "3. ‚úÖ Add more training data (aim for 2000+ examples)\n",
    "\n",
    "**For Faster Training (If you have VRAM):**\n",
    "1. ‚úÖ Increase batch_size to 4 or 8\n",
    "2. ‚úÖ Slightly increase learning_rate to 1e-4\n",
    "\n",
    "**For Production Quality:**\n",
    "1. ‚úÖ All of the above\n",
    "2. ‚úÖ Implement validation split to monitor overfitting\n",
    "3. ‚úÖ Use learning rate scheduling (warmup + decay)\n",
    "4. ‚úÖ Test multiple LoRA ranks and pick the best\n",
    "\n",
    "### üéØ Pro Tips\n",
    "\n",
    "1. **Start small**: Try one change at a time to see what works\n",
    "2. **Monitor GPU memory**: Use `nvidia-smi` to check you're not hitting limits\n",
    "3. **Validate results**: Test on questions NOT in your training data\n",
    "4. **Save checkpoints**: Save your model after each experiment\n",
    "5. **Track metrics**: Keep a spreadsheet of what works and what doesn't\n",
    "6. **Watch the loss curve**: Loss should decrease smoothly - if it spikes, reduce learning rate\n",
    "7. **Compare templates**: Test both Gemma and Custom templates after each change\n",
    "\n",
    "**Remember**: The best configuration depends on your specific use case, dataset, and hardware! üéØ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
